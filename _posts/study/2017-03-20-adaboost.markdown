---
layout: post
title:  AdaBoost算法
author: "R. Liao" 
categories: study
tags: AdaBoost
---


Boosting是一种提高任意给定学习算法准确度的方法，是一种常用的统计学习方法，应用广泛且有效。

* 在分类问题中， 它通过改变训练样本的权重，学习多个分器，并将这些分类器进行组合，提高分类性能。
* 提升方法中最具代表性的算法是 **Adaboost**。

#### 基本思路

* 对于一个复杂任务，将多个专家的判断进行适当的
综合所得出的判断，要比其中任何一个专家单独的
判断要好。

* 这就是“三个臭皮匠顶个诸葛亮” 。


#### 处理分类问题的思想

给定训练集， 寻找比较粗糙的分类规则（弱分类器）
要比寻找精确的分类规则要简单得多。

* 提升算法的核心是从弱学习算法出发，反复学习，得
到一系列弱分类器。然后组合这些弱分类器，构成一
个强分类器。

* 基本做法： 改变训练数据的概率（权重）分布，针对
不同的训练数据的分布，调用弱学习算法来学习一系
列分类器。

**需要回答两个问题**

1.在每轮训练中，如何改变训练数据的权值或分布？

2.如何将一系列的弱分类器组合成一个强分类器？

#### 关于第一个问题

* Adaboost的做法是：提高那些被前一轮弱分类器分
错的样本的权重，降低已经被正确分类的样本的权
重。 错分的样本将在下一轮弱分类器中得到更多关
注。于是分类问题被一系列弱分类器“分而治之” 。


#### 关于第二个问题

* 关于弱分类器的组合， Adaboost的做法是：采用加
权(多数)表决的方法。具体地，加大分类误差率较
小的弱分类器的权重，使其在表决中起更大的作用。


**Adaboost的巧妙之处在于将这些想法融合于一个算法之中！**


#### 对Adaboost算法的说明

 AdaBoost算法中不同的训练集是通过调整每个样本对应的权重来实现的。
 
 开始时，每个样本对应的权重是相同的，即其中n为样本个数，在此样本分布下训练出一弱分类器。
 
 对于分类错误的样本，加大其对应的权重；而对于分类正确的样本，降低其权重，这样分错的样本就被突显出来，从而得到一个新的样本分布。
 
 在新的样本分布下，再次对样本进行训练，得到弱分类器。
 
 依次类推，经过T次循环，得到T个弱分类器，把这T个弱分类器按一定的权重叠加（boost）起来，得到最终想要的强分类器。  

#### Adaboost的核心思想

“关注”被错分的样本，“器重”性能好的弱分类器

1. 不同的训练集,调整样本权重
2. “关注” 增加错分样本权重
3. “器重” 好的分类器权重大
4. 样本权重间接影响分类器权重

####  Adaboost 算法步骤

AdaBoost算法的具体步骤如下： 　　

1. 给定训练样本集S，其中X和Y分别对应于正例样本和负例样本；T为训练的最大循环次数；

2. 初始化样本权重为1/n ，即为训练样本的初始概率分布； 　　

3. 第一次迭代：

     1. 训练样本的概率分布相当，训练弱分类器;

     2.    计算弱分类器的错误率;

     3.   选取合适阈值，使得误差最小；

     4.    更新样本权重； 

 经T次循环后，得到T个弱分类器，按更新的权重叠加，最终得到的强分类器。 


####  代码实现

可以点击[链接](https://github.com/Jarily/AdaBoost)查看测试数据和源代码


*  基于Python的单层决策树作为弱分类器

~~~
# -*- coding: utf-8 -*-
"""
Created on Wed Mar 29 14:19:42 2017

@author: Jarily
"""

import numpy as np

'''
Decision Stump  单层决策树算法   弱分类器
'''

class DecisionStump:
    def __init__(self,X,y):
        self.X=np.array(X)
        self.y=np.array(y)
        self.N=self.X.shape[0]
        

    
    def train(self,W,steps=100):  #返回所有参数中阈值最小的
        '''    
        W长度为N的向量,表示N个样本的权值  
        threshold_value为阈值
        threshold_pos为第几个参数
        threshold_tag为1或者-1.大于阈值则分为threshold_tag，小于阈值则相反
        '''
        min = float("inf")    #将min初始化为无穷大
        threshold_value=0;
        threshold_pos=0;
        threshold_tag=0;
        self.W=np.array(W)
        for i in range(self.N):  #  value表示阈值，errcnt表示错误的数量
            value,errcnt = self.findmin(i,1,steps)
            if (errcnt < min):
                min = errcnt
                threshold_value = value
                threshold_pos = i
                threshold_tag = 1
        for i in range(self.N):  # -1
            value,errcnt= self.findmin(i,-1,steps)
            if (errcnt < min):
                min = errcnt
                threshold_value = value
                threshold_pos = i
                threshold_tag = -1
        #最终更新
        self.threshold_value=threshold_value
        self.threshold_pos=threshold_pos
        self.threshold_res=threshold_tag
        #print(self.threshold_value,self.threshold_pos,self.threshold_res) 
        return min
    
    def findmin(self,i,tag,steps):  #找出第i个参数的最小的阈值,tag为1或-1
        t = 0
        tmp = self.predintrain(self.X,i,t,tag).transpose()
        errcnt = np.sum((tmp!=self.y)*self.W)
        #print now
        buttom=np.min(self.X[i,:])  #该项属性的最小值，下界
        up=np.max(self.X[i,:])      #该项属性的最大值，上界
        minerr = float("inf")       #将minerr初始化为无穷大
        value=0                     #value表示阈值
        st=(up-buttom)/steps        #间隔
        for t in np.arange(buttom,up,st):
            tmp = self.predintrain(self.X,i,t,tag).transpose()
            #print now.shape,self.W.shape,(now!=self.y).shape,self.y.shape
            errcnt = np.sum((tmp!=self.y)*self.W)
            if errcnt < minerr:
                minerr=errcnt
                value=t
        return value,minerr
    
    def predintrain(self,test_set,i,t,tag): #训练时按照阈值为t时预测结果
        test_set=np.array(test_set).reshape(self.N,-1)
        pre_y = np.ones((np.array(test_set).shape[1],1))
        pre_y[test_set[i,:]*tag<t*tag]=-1
        return pre_y

    def pred(self,test_X):  #弱分类器的预测
        test_X=np.array(test_X).reshape(self.N,-1) #转换为N行X列，-1懒得算
        pre_y = np.ones((np.array(test_X).shape[1],1))
        pre_y[test_X[self.threshold_pos,:]*self.threshold_res<self.threshold_value*self.threshold_res]=-1
        return pre_y
	
~~~



*  基于的Python的ADaBoost算法

~~~
# -*- coding: utf-8 -*-
"""
Created on Wed Mar 29 14:19:42 2017

@author: Jarily
"""
# coding: UTF-8

import numpy as np
from WeakClassify import DecisionStump


class AdaBoost:
    def __init__(self,X,y,Weaker=DecisionStump):
        self.X=np.array(X)
        self.y=np.array(y).flatten(1)
        self.Weaker=Weaker
        self.sums=np.zeros(self.y.shape)
        
        '''
        W为权值，初试情况为均匀分布，即所有样本都为1/n
        '''
        self.W=np.ones((self.X.shape[1],1)).flatten(1)/self.X.shape[1]

        self.Q=0  #弱分类器的实际个数
        
       # M 为弱分类器的最大数量，可以在main函数中修改
        
    def train(self,M=5):
        self.G={}         # 表示弱分类器的字典
        self.alpha={}     # 每个弱分类器的参数
        for i in range(M):
            self.G.setdefault(i)
            self.alpha.setdefault(i)
        for i in range(M):   # self.G[i]为第i个弱分类器
            self.G[i]=self.Weaker(self.X,self.y)
            e=self.G[i].train(self.W) #根据当前权值进行该个弱分类器训练
            self.alpha[i]=1/2*np.log((1-e)/e) #计算该分类器的系数
            res=self.G[i].pred(self.X)  #res表示该分类器得出的输出
            # Z表示规范化因子
            Z=self.W*np.exp(-self.alpha[i]*self.y*res.transpose())
            self.W=(Z/Z.sum()).flatten(1) #更新权值
            self.Q=i
            # errorcnt返回分错的点的数量，为0则表示perfect
            if (self.errorcnt(i)==0):
                print("%d个弱分类器可以将错误率降到0"%(i+1))
                break
            

    def errorcnt(self,t):   #返回错误分类的点
        self.sums=self.sums+self.G[t].pred(self.X).flatten(1)*self.alpha[t]
        
        pre_y=np.zeros(np.array(self.sums).shape)
        pre_y[self.sums>=0]=1
        pre_y[self.sums<0]=-1
        
        t=(pre_y!=self.y).sum() 
        return t
    
    def pred(self,test_X):  #测试最终的分类器
        test_X=np.array(test_X)
        sums=np.zeros(test_X.shape[1])
        for i in range(self.Q+1):
            sums=sums+self.G[i].pred(test_X).flatten(1)*self.alpha[i]
        pre_y=np.zeros(np.array(sums).shape)
        pre_y[sums>=0]=1
        pre_y[sums<0]=-1
        return pre_y

~~~
